{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7aa4b6053f8940a98db05406204ea274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_76b6f3af0f774602927404f77bf7e5f4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8d400680c5fd40ed81d529cb5a12c578",
              "IPY_MODEL_e6b636cbb287403d9e136302fc067b90"
            ]
          }
        },
        "76b6f3af0f774602927404f77bf7e5f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d400680c5fd40ed81d529cb5a12c578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ebc56d2356894970890417a7ac45882e",
            "_dom_classes": [],
            "description": "Training - Epoch 1/1: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 50640,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50640,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad855b33c90b4efba19c3499b7d79df3"
          }
        },
        "e6b636cbb287403d9e136302fc067b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_abe9738aa23d425fab98b2aab998585d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50880/? [02:05&lt;00:00, 404.97 imgs/s, Epoch loss=0.0376, Epoch valid pairs=0, Percentage of train set=1]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c2914112dd6491fb23cb1eb13addd2e"
          }
        },
        "ebc56d2356894970890417a7ac45882e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad855b33c90b4efba19c3499b7d79df3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "abe9738aa23d425fab98b2aab998585d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c2914112dd6491fb23cb1eb13addd2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bfaaa229c1f34aa9b207ecdb83b13c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2f4a401fe7a847ec9c98ea3c3625d2b3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_378dd59f748e47169d8c3117e721885b",
              "IPY_MODEL_8f48b26764014093a3c7ed79197ae3d7"
            ]
          }
        },
        "2f4a401fe7a847ec9c98ea3c3625d2b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "378dd59f748e47169d8c3117e721885b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ca83ee532cd4486bb0bfd093fd662668",
            "_dom_classes": [],
            "description": "Validation - Epoch 1/1: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21f3d3b612f04c879d4e7bc19f5c6a8c"
          }
        },
        "8f48b26764014093a3c7ed79197ae3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_779e8ab4ac74413490933519ea8ebeec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/0 [00:00&lt;?, ? imgs/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1b0817e1eb14342a4c353ac366e9c1d"
          }
        },
        "ca83ee532cd4486bb0bfd093fd662668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21f3d3b612f04c879d4e7bc19f5c6a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "779e8ab4ac74413490933519ea8ebeec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1b0817e1eb14342a4c353ac366e9c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLCGvlIl6j5N"
      },
      "source": [
        "# Import and Installs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHpCXe-QUpki"
      },
      "source": [
        "# colab commands for installing missing modules\n",
        "!pip install mtcnn\n",
        "!pip install tensorflow_addons\n",
        "\n",
        "# Imports\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "import os\n",
        "from scipy import interpolate\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn import metrics\n",
        "from scipy.optimize import brentq\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.losses import metric_learning\n",
        "import datetime\n",
        "import mtcnn\n",
        "import cv2\n",
        "from google.colab import files, drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiehnWdf_ejf"
      },
      "source": [
        "# Remove colab's sample data folder\n",
        "!rm -rf sample_data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU6awAEdqQcq"
      },
      "source": [
        "##Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNlWUgdLqPla"
      },
      "source": [
        "# Hyperparameters and constants\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "IMG_SIZE = 224                  # 224 for mobilenet, 299 for InceptionV3\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "DIR = 'CASIA-WebFace_mtcnnpy'\n",
        "MARGIN = 32       \n",
        "NUM_EPOCHS = 100\n",
        "LFW_PAIRS_PATH = r'pairs.txt'\n",
        "LFW_DIR = r'lfw_mtcnn'\n",
        "TRAINLOSS = 'semiHardAngular'   # all, allAngular, semiHard, semiHardAngular\n",
        "VALLOSS = 'all'                 # validation loss \n",
        "BACKBONE = 'MobileNetV3Small'   \n",
        "NUM_OF_IDS = 40                 # how many different identities per batch\n",
        "IMAGES_PER_PERSON = 6           # how many images per identity\n",
        "CKPT_DIR = os.path.join('./checkpoints')    # Best .hdf5 model storage\n",
        "\n",
        "# Specify which datasets should be downloaded\n",
        "datasetsDownload = {'lfw':False, 'lfwCropped':True, 'casia':False, 'casiaCropped':True}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0dJPbiCcXuG"
      },
      "source": [
        "#Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgH0UkCocYx9"
      },
      "source": [
        "@tf.function\n",
        "def cosineLoss(labels, embeddings, margin, squared=False):\n",
        "    # Get the distance matrix which hold the distance of each pair of images\n",
        "    distances = _cosDist(embeddings)\n",
        "\n",
        "    # Expand dimensions in preparation for triplet disntaces and assert propper shapes\n",
        "    anchorToPos = tf.expand_dims(distances, 2)\n",
        "    assert anchorToPos.shape[2] == 1, \"Invalid shape \" + str(anchorToPos.shape)\n",
        "    anchorToNeg = tf.expand_dims(distances, 1)\n",
        "    assert anchorToNeg.shape[1] == 1, \"Invalid shape \" + str(anchorToNeg.shape)\n",
        "\n",
        "    # Compute distances of triplets: tripletLoss[i, j, k] = anchor=i, positive=j, negative=k\n",
        "    tripletLoss = anchorToPos - anchorToNeg + margin\n",
        "\n",
        "    # Create a matrix of triplet validity, valid triplets should have anchor and positive of the same ID and negative of different ID\n",
        "    validityMask = _getTripletValidity(labels)\n",
        "    validityMask = tf.cast(validityMask,tf.float32)\n",
        "\n",
        "    #Multiply the validty mask with the loss, therefore retaining loss only for valid triplets\n",
        "    tripletLoss = tf.multiply(validityMask, tripletLoss)\n",
        "\n",
        "    # Easy triplets generate negative loss, remove these\n",
        "    tripletLoss = tf.maximum(0., tripletLoss)\n",
        "\n",
        "    # Calculate how many triplets still generate loss:\n",
        "    lossyTriplets = tf.reduce_sum(tf.cast(tf.greater(tripletLoss, 1e-16),tf.float32))\n",
        "    lossyTripletsCount = tf.reduce_sum(validityMask)\n",
        "    lossyTripletsFrac = lossyTriplets / (lossyTripletsCount + 1e-16)\n",
        "\n",
        "    # Get final mean triplet loss over the positive valid triplets\n",
        "    tripletLoss = tf.reduce_sum(tripletLoss) / (lossyTriplets + 1e-16)\n",
        "\n",
        "    return tripletLoss\n",
        "\n",
        "\n",
        "# Calculates the angle between each two embeddings of the e parameter\n",
        "@tf.function\n",
        "def _cosDist(e):\n",
        "    d1 = tf.expand_dims(e, 0)\n",
        "    d1 = tf.repeat(d1, tf.shape(d1)[1], axis=0)\n",
        "    d2 = tf.transpose(e)\n",
        "    d2 = tf.expand_dims(e, 1)\n",
        "    d2 = tf.repeat(d2, tf.shape(d2)[0], axis=1)\n",
        "    distances = _angle(d1, d2)\n",
        "    return distances\n",
        "\n",
        "\n",
        "# Calculates the angle between two embeddings\n",
        "@tf.function\n",
        "def _angle(e1, e2):\n",
        "    up = tf.multiply(e1, e2)\n",
        "    up = tf.reduce_sum(up, axis=2)\n",
        "    down = tf.tensordot(tf.norm(e1, axis=0), tf.norm(e2, axis=0),2)\n",
        "    # the embeddings are already normalized, just divide by the sum of their squared values (always 2)\n",
        "    res = up / 2. \n",
        "    # modify the range to <2, 0>\n",
        "    return 1. - res \n",
        "   \n",
        "\n",
        "# Returns a matrix, which hold True where the triplet is valid (eg. same positive Id, different negative Id as the anchor)\n",
        "# Returns False where the IDs do not match the above condition or the triplet contains identical images\n",
        "@tf.function\n",
        "def _getTripletValidity(labels):\n",
        "    # Check that i, j and k are distinct\n",
        "    equalIndices = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
        "    nonEqualIndices = tf.logical_not(equalIndices)\n",
        "    # Create 3 matrices, each contains the informaton about equal images\n",
        "    i2j = tf.expand_dims(nonEqualIndices, 2)\n",
        "    i2k = tf.expand_dims(nonEqualIndices, 1)\n",
        "    j2k = tf.expand_dims(nonEqualIndices, 0)\n",
        "\n",
        "    # The valid triplets should contain different images - all images should be nonequal\n",
        "    validTriplets = tf.logical_and(tf.logical_and(i2j, i2k), j2k)\n",
        "\n",
        "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    sameLabels = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "    i2j = tf.expand_dims(sameLabels, 2)\n",
        "    i2k = tf.expand_dims(sameLabels, 1)\n",
        "\n",
        "    correctClasses = tf.logical_and(i2j, tf.logical_not(i2k))\n",
        "\n",
        "    # Combine the two conditions\n",
        "    validity = tf.logical_and(validTriplets, correctClasses)\n",
        "\n",
        "    return validity\n",
        "\n",
        "@tf.function\n",
        "def tripletLoss(labels, embeddings, margin, squared=False):\n",
        "    # Get the distance matrix which hold the distance of each pair of images\n",
        "    distances = metric_learning.pairwise_distance(embeddings, squared=False)\n",
        "\n",
        "    # Expand dimensions in preparation for triplet disntaces and assert propper shapes\n",
        "    anchorToPos = tf.expand_dims(distances, 2)\n",
        "    assert anchorToPos.shape[2] == 1, \"Invalid shape \" + str(anchorToPos.shape)\n",
        "    anchorToNeg = tf.expand_dims(distances, 1)\n",
        "    assert anchorToNeg.shape[1] == 1, \"Invalid shape \" + str(anchorToNeg.shape)\n",
        "\n",
        "    # Compute distances of triplets: tripletLoss[i, j, k] = anchor=i, positive=j, negative=k\n",
        "    tripletLoss = anchorToPos - anchorToNeg + margin\n",
        "\n",
        "    # Create a matrix of triplet validity, valid triplets should have anchor and positive of the same ID and negative of different ID\n",
        "    validityMask = _getTripletValidity(labels)\n",
        "    validityMask = tf.cast(validityMask,tf.float32)\n",
        "\n",
        "    #Multiply the validty mask with the loss, therefore retaining loss only for valid triplets\n",
        "    tripletLoss = tf.multiply(validityMask, tripletLoss)\n",
        "\n",
        "    # Easy triplets generate negative loss, remove these\n",
        "    tripletLoss = tf.maximum(0., tripletLoss)\n",
        "\n",
        "    # Calculate how many triplets still generate loss:\n",
        "    lossyTriplets = tf.reduce_sum(tf.cast(tf.greater(tripletLoss, 1e-16),tf.float32))\n",
        "    lossyTripletsCount = tf.reduce_sum(validityMask)\n",
        "    lossyTripletsFrac = lossyTriplets / (lossyTripletsCount + 1e-16)\n",
        "\n",
        "    # Get final mean triplet loss over the positive valid triplets\n",
        "    tripletLoss = tf.reduce_sum(tripletLoss) / (lossyTriplets + 1e-16)\n",
        "\n",
        "    return tripletLoss, lossyTripletsFrac\n",
        "\n",
        "# Strips the fraction return value\n",
        "@tf.function\n",
        "def tripletLossNoVal(labels, embeddings, margin, squared=False):\n",
        "    loss, fraction = tripletLoss(labels, embeddings, margin, squared=False)\n",
        "    return loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frYeNRk9fzwZ"
      },
      "source": [
        "Initialize losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYoY1Grxf1U6"
      },
      "source": [
        "# all, allAngular, semiHard, semiHardAngular\n",
        "# Training:\n",
        "if TRAINLOSS == 'all': \n",
        "  trainLoss = tripletLossNoVal                                                             # Full batch, euclidean\n",
        "elif TRAINLOSS == 'allAngular':\n",
        "  trainLoss = cosineLoss                                                                   # Full batch, cosine\n",
        "elif TRAINLOSS == 'semiHard':\n",
        "  trainLoss = tfa.losses.TripletSemiHardLoss(margin = 0.2)                                 # Semi hards, euclidean\n",
        "elif TRAINLOSS == 'semiHardAngular':\n",
        "  trainLoss = tfa.losses.TripletSemiHardLoss(margin = 0.2, distance_metric = \"angular\")    # Semi hards, cosine\n",
        "else:\n",
        "  raiseException(\"Train loss hyperparameter not recognized:\", TRAINLOSS)\n",
        "\n",
        "# Validation:\n",
        "if VALLOSS == 'all': \n",
        "  valLoss = tripletLoss                                                                  # Full batch, euclidean\n",
        "elif VALLOSS == 'allAngular':\n",
        "  valLoss = cosineLoss                                                                   # Full batch, cosine\n",
        "elif VALLOSS == 'semiHard':\n",
        "  valLoss = tfa.losses.TripletSemiHardLoss(margin = 0.2)                                 # Semi hards, euclidean\n",
        "elif VALLOSS == 'semiHardAngular':\n",
        "  valLoss = tfa.losses.TripletSemiHardLoss(margin = 0.2, distance_metric = \"angular\")    # Semi hards, cosine\n",
        "else:\n",
        "  raiseException(\"Validation loss hyperparameter not recognized:\", VALLOSS)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQIsC6YnVovC"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZt93chZTW8Y"
      },
      "source": [
        "@tf.function\n",
        "def processImage(imgPath,label=None):\n",
        "  # load the raw data from the file\n",
        "  img = tf.io.read_file(imgPath)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "\n",
        "  # Convert to tf float and resize to IMG_SIZE\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  img = tf.image.resize(img, (IMG_SIZE,IMG_SIZE))\n",
        "  if label is not None:\n",
        "    return img, label\n",
        "  else:\n",
        "      return img\n",
        "\n",
        "\n",
        "# Set resolution to IMG_SHAPE\n",
        "def setShapes(image, label=None):\n",
        "  image.set_shape(IMG_SHAPE)\n",
        "  if label is not None:\n",
        "    label.set_shape([])\n",
        "    return image, label\n",
        "  else:\n",
        "    return image\n",
        "\n",
        "\n",
        "# Augments the 'image' with flip, jpeg quality, brightness, contranst and saturation\n",
        "def augment(image, label, flipLr=True, quality=False, brightness=True, contrast=True, saturation=True):\n",
        "  if flipLr:\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "  if quality:\n",
        "    image = tf.image.random_jpeg_quality(image,80,100)\n",
        "\n",
        "  if brightness:\n",
        "    image = tf.image.random_brightness(image, 0.2)\n",
        "\n",
        "  if contrast:\n",
        "    image = tf.image.random_contrast(image, 0.2, 0.5)      \n",
        "\n",
        "  if saturation:\n",
        "    image = tf.image.random_saturation(image, 5, 10)  \n",
        "\n",
        "  return image, label\n",
        "\n",
        "\n",
        "# Creates generator containing k images from each identity in df\n",
        "def permutateDS(df, k):\n",
        "    uniqueLabels = np.random.permutation(df.label.unique())     # Get all labels (once/unique) and shuffle them\n",
        "    for currentLabel in uniqueLabels:                                  # For each identity\n",
        "        sameIdImages = df[df.label == currentLabel].sample(n=k)  # Keep rows with label == currentLabel and select k samples from it\n",
        "        for i in range(k):\n",
        "            image = sameIdImages.index[i]\n",
        "            identity = sameIdImages.label[i]\n",
        "            yield image, identity\n",
        "    return\n",
        "\n",
        "\n",
        "# Object holding all train and validation data, as well as the tf dataset format generated from it\n",
        "# LabelsPerBatch - how many different people should be included in batch\n",
        "# MinIdOccurence - how many images per person should be in batch\n",
        "class TripletDataset:\n",
        "    def __init__(self, labelsPerBatch, minIdOccurence):\n",
        "        self.trainData      = None\n",
        "        self.valData        = None\n",
        "        self.minIdOccurence = minIdOccurence\n",
        "        self.labelsPerBatch = labelsPerBatch\n",
        "        self.trainDataset   = None\n",
        "        self.valDataset     = None\n",
        "\n",
        "\n",
        "    # Loads the CASIA-webface dataset (location specified by 'dir' parameter) into a dataframe\n",
        "    def loadCasia(self, dir, isValidation=False):\n",
        "        # Load images into pandas dataframe. Image folders are labels in this case\n",
        "        images = glob(os.path.join(dir,'*/*.jpg'))            # image paths\n",
        "        labels = [path.split('/')[-2] for path in images]     # folders\n",
        "        data = pd.DataFrame(labels, index=images, columns=['label'])    \n",
        "\n",
        "        # Filter indentities with fewer images than minIdOccurence\n",
        "        occurences = data['label'].value_counts()\n",
        "        lowCountIds = occurences[occurences < self.minIdOccurence]\n",
        "        data = data[~data['label'].isin(lowCountIds.index)]\n",
        "        data.label = data.label.astype('category').cat.codes\n",
        "\n",
        "        # Save the data as either validation or train dataset\n",
        "        if isValidation:\n",
        "            self.valData = data\n",
        "        else:\n",
        "            self.trainData = data\n",
        "\n",
        "\n",
        "    # Splits the dataset into train/val parts with the given ratio\n",
        "    def splitDataset(self, ratio = 0.2):\n",
        "        np.random.seed(42)\n",
        "        valLabels       = np.random.permutation(self.trainData.label.nunique()) #select random labels for val dataset\n",
        "        self.trainData  = self.trainData[self.trainData.label.isin(valLabels[int(ratio * len(valLabels)):])]\n",
        "        self.valData    = self.trainData[self.trainData.label.isin(valLabels[:int(ratio * len(valLabels))])]\n",
        "        \n",
        "\n",
        "    # maps all the preprocessing functions to the dataset\n",
        "    def generateDataset(self, augmentImg=True):\n",
        "        # Get generators \n",
        "        self.trainGen     = partial(permutateDS, df = self.trainData, k = self.minIdOccurence)\n",
        "        self.valGen       = partial(permutateDS, df = self.valData, k = self.minIdOccurence)\n",
        "\n",
        "        # Get datasets from generator\n",
        "        self.trainDataset = tf.data.Dataset.from_generator(self.trainGen, (tf.string, tf.int32))\n",
        "        self.valDataset   = tf.data.Dataset.from_generator(self.valGen, (tf.string, tf.int32))\n",
        "        \n",
        "        # map preprocessing functions and make batches\n",
        "        self.trainDataset = self.trainDataset.map(processImage, num_parallel_calls = AUTOTUNE)\n",
        "        self.trainDataset = self.trainDataset.map(setShapes, num_parallel_calls = AUTOTUNE)\n",
        "        self.trainDataset = self.trainDataset.batch(self.batchSize())\n",
        "\n",
        "        # Augment \n",
        "        if augmentImg:\n",
        "            self.trainDataset = self.trainDataset.map(augment, num_parallel_calls = AUTOTUNE)\n",
        "        self.trainDataset = self.trainDataset.prefetch(buffer_size = AUTOTUNE)\n",
        "        \n",
        "        # Map preprocess for validation\n",
        "        self.valDataset   = self.valDataset.map(processImage, num_parallel_calls = AUTOTUNE)\n",
        "        self.valDataset   = self.valDataset.map(setShapes, num_parallel_calls = AUTOTUNE)\n",
        "        self.valDataset   = self.valDataset.batch(self.batchSize())\n",
        "        self.valDataset   = self.valDataset.prefetch(buffer_size = AUTOTUNE)\n",
        "        \n",
        "\n",
        "    # Returns batch size (int). \n",
        "    def batchSize(self):\n",
        "        return self.minIdOccurence * self.labelsPerBatch\n",
        "\n",
        "\n",
        "    # Calculates how many steps are in epoch.\n",
        "    def stepsInEpoch(self):\n",
        "        if self.valData is not None:\n",
        "            return self.trainData.label.nunique() // self.labelsPerBatch, self.valData.label.nunique() // self.labelsPerBatch\n",
        "        else:\n",
        "            return self.trainData.label.nunique() // self.labelsPerBatch, 0  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehiw-C8TpMfG"
      },
      "source": [
        "Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCNSL-3wClJY"
      },
      "source": [
        "# Download the LFW dataset (unmodified)\n",
        "if datasetsDownload['lfw'] and not os.path.isfile(\"lfw.tgz\"):\n",
        "  !wget http://vis-www.cs.umass.edu/lfw/lfw.tgz               \n",
        "  !tar -xzf lfw.tgz\n",
        "# Download the Casia dataset (unmodified)  \n",
        "if datasetsDownload['casia'] and not os.path.isfile('casia.zip'):\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Of_EVz-yHV7QVWQGihYfvtny9Ne8qXVz' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Of_EVz-yHV7QVWQGihYfvtny9Ne8qXVz\" -O casia.zip && rm -rf /tmp/cookies.txt\n",
        "  !unzip -q casia.zip\n",
        "# Download the cropped LFW dataset  \n",
        "if datasetsDownload['lfwCropped'] and not os.path.isfile(\"lfw_mtcnn.zip\"):\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R2wJDuZIxu1Rtx4Ei05cbokUTP0EV2YQ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R2wJDuZIxu1Rtx4Ei05cbokUTP0EV2YQ\" -O lfw_mtcnn.zip && rm -rf /tmp/cookies.txt\n",
        "  !unzip -q lfw_mtcnn.zip\n",
        "# Download the cropped CASIA dataset  \n",
        "if datasetsDownload['casiaCropped'] and not os.path.isfile(\"CASIA-WebFace_mtcnn.zip\"):\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1C6zmd0eBrFZzDnenio44-W6XAlmteoW3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1C6zmd0eBrFZzDnenio44-W6XAlmteoW3\" -O CASIA-WebFace_mtcnn.zip && rm -rf /tmp/cookies.txt\n",
        "  !unzip -q CASIA-WebFace_mtcnn.zip \n",
        "\n",
        "# Download the pairs.txt file from lfw website for evaluation\n",
        "if not os.path.isfile('pairs.txt'):\n",
        "  !wget http://vis-www.cs.umass.edu/lfw/pairs.txt\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMa-WuDemvja"
      },
      "source": [
        "#Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th9YSgLicjpa"
      },
      "source": [
        "# Constructs a NN architecture based on specified parameters:\n",
        "# backbone      - specifies the CNN for feature extraction. This NN is initialized from imagenet (classfication) weights.\n",
        "#               - The top layers of the backbone are excluded\n",
        "# embeddingSize - Specifies the size of the output layer (and therefore the size of embedding)\n",
        "# fcSize        - The size of the intermediate fully connected layer\n",
        "# l2Norm        - Whether to add l2 normalization layer\n",
        "def getNetwork(backbone = 'ResNet50V2', embeddingSize=128, fcSize=1024, l2Norm=True):\n",
        "    # Get backbone\n",
        "    if backbone == 'ResNet50V2':\n",
        "      baseModel = tf.keras.applications.ResNet50V2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "    elif backbone == 'ResNet101V2':\n",
        "      baseModel = tf.keras.applications.ResNet101V2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "    elif backbone == 'ResNet152V2':\n",
        "      baseModel = tf.keras.applications.ResNet152V2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet') \n",
        "    elif backbone == 'InceptionV3':\n",
        "      baseModel = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')  \n",
        "    elif backbone == 'MobileNetV3Large':\n",
        "      baseModel = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "    elif backbone == 'MobileNetV3Small':\n",
        "      baseModel = tf.keras.applications.MobileNetV3Small(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "    elif backbone == 'DenseNet169':\n",
        "      baseModel = tf.keras.applications.DenseNet169(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "    elif backbone == 'DenseNet121':\n",
        "      baseModel = tf.keras.applications.DenseNet121(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "    elif backbone == 'InceptionResNetV2':\n",
        "      baseModel = tf.keras.applications.InceptionResNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "    elif backbone == 'EfficientNetB3':\n",
        "      baseModel = tf.keras.applications.EfficientNetB3(input_shape=IMG_SHAPE, include_top=False, weights='imagenet') \n",
        "    else:\n",
        "      raise Exception(\"Backbone network not matched to any known architecture:\", backbone)\n",
        "\n",
        "    # Set CNN to be trainable\n",
        "    baseModel.trainable = True\n",
        "\n",
        "    # Create a sequential model with backbone and new top layers\n",
        "    model = tf.keras.Sequential([\n",
        "        baseModel,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),   # pools accross whole image, retains the filter dimension (eg. (7,7,256) -> (256))\n",
        "        tf.keras.layers.Dense(fcSize, activation='relu'),   # intermediate dense layer \n",
        "        tf.keras.layers.BatchNormalization(),                 \n",
        "        tf.keras.layers.Dense(embeddingSize), ])            # output layer\n",
        "        \n",
        "    # Add l2 normalization as lambda layer (recommended)    \n",
        "    if l2Norm:\n",
        "        model.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdr7rv4uKpOw"
      },
      "source": [
        "#Align faces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnuqmqFrKzN3"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "# The cropped image should have MARGIN pixels around the face in each direction.\n",
        "# If the original image does not have sufficient pixels on given side, the side of the original image\n",
        "# is selected as the new margin border.\n",
        "\n",
        "detector = mtcnn.MTCNN()\n",
        "\n",
        "images = glob(os.path.join(CROPPEDDIR, DIR, '*/*.jpg'))\n",
        "\n",
        "with tqdm(desc=f'Resizing', unit=' imgs', total=494414 ) as pbar:\n",
        "  for imgPath in images:\n",
        "    img = cv2.imread(imgPath)\n",
        "    result = detector.detect_faces(img)\n",
        "    if len(result) > 0:\n",
        "      bounding_box = result[0]['box']\n",
        "      x = np.max(bounding_box[0]-(MARGIN//2),0)\n",
        "      y = np.max(bounding_box[1]-(MARGIN//2),0) \n",
        "      x2 = np.minimum(bounding_box[0]+bounding_box[2]+(MARGIN//2),img.shape[0])\n",
        "      y2 = np.minimum(bounding_box[1]+bounding_box[3]+(MARGIN//2),img.shape[1])\n",
        "      img = img[y:y2, x:x2]\n",
        "      print(\"Successfull crop\", imgPath)\n",
        "    else:\n",
        "      print('Cant crop', imgPath)\n",
        "    print(os.path.join(CROPPEDDIR, imgPath))  \n",
        "    print(cv2.imwrite(os.path.join(CROPPEDDIR, imgPath), img))\n",
        "    pbar.update(1)  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gwVlG8Xm1J-"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG3EQXEzpOX3"
      },
      "source": [
        "Initialize training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFYi5fYOpHRl",
        "outputId": "b0f3543c-a7fe-44fd-ac77-d21e1dfa21e1"
      },
      "source": [
        "# Create dir for checkpoints\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "# Create model and print its architecture\n",
        "model = getNetwork(backbone = BACKBONE, embeddingSize=128, fcSize=1024, l2Norm=True)\n",
        "model.summary()\n",
        "\n",
        "# Create optimizer, for adam use lr between 0.0002 and 0.00005\n",
        "optimizer=tf.keras.optimizers.Adam(lr=0.0001)\n",
        "\n",
        "# Create dataset\n",
        "dataset = TripletDataset(NUM_OF_IDS,IMAGES_PER_PERSON)\n",
        "dataset.loadCasia(DIR)\n",
        "dataset.splitDataset()                        # You can set optional split ratio, default is 80/20\n",
        "dataset.generateDataset(augmentImg = False)   # Augmentation did not bring improvement"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top.h5\n",
            "6701056/6698480 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "MobilenetV3small (Functional (None, 7, 7, 1024)        1529968   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 128)               0         \n",
            "=================================================================\n",
            "Total params: 2,714,864\n",
            "Trainable params: 2,700,704\n",
            "Non-trainable params: 14,160\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZM-9DUcpLJS"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BcNphHtaCzX"
      },
      "source": [
        "# Single training step. Runs batch forward through NN, calculates loss and adjusts the weights.\n",
        "# margin - tripletLoss margin \n",
        "@tf.function\n",
        "def trainStep(X, y, model, optimizer, margin=0.2):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # pass the images forward through the network\n",
        "        embeddings = model(X, training=True)\n",
        "        # calculate loss\n",
        "        loss = trainLoss(y, embeddings, margin)\n",
        "        # apply gradients\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss, 0\n",
        "\n",
        "# Single validation step. You might need to modedify this if you wish to change the validation metrics\n",
        "@tf.function\n",
        "def valStep(X, y, model, margin=0.2):\n",
        "    embeddings = model(X)\n",
        "    loss, fraction = valLoss(y, embeddings, margin)\n",
        "    #loss = valLoss(y, embeddings, margin)\n",
        "    #fraction = 0\n",
        "    return loss, fraction"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186,
          "referenced_widgets": [
            "7aa4b6053f8940a98db05406204ea274",
            "76b6f3af0f774602927404f77bf7e5f4",
            "8d400680c5fd40ed81d529cb5a12c578",
            "e6b636cbb287403d9e136302fc067b90",
            "ebc56d2356894970890417a7ac45882e",
            "ad855b33c90b4efba19c3499b7d79df3",
            "abe9738aa23d425fab98b2aab998585d",
            "5c2914112dd6491fb23cb1eb13addd2e",
            "bfaaa229c1f34aa9b207ecdb83b13c23",
            "2f4a401fe7a847ec9c98ea3c3625d2b3",
            "378dd59f748e47169d8c3117e721885b",
            "8f48b26764014093a3c7ed79197ae3d7",
            "ca83ee532cd4486bb0bfd093fd662668",
            "21f3d3b612f04c879d4e7bc19f5c6a8c",
            "779e8ab4ac74413490933519ea8ebeec",
            "e1b0817e1eb14342a4c353ac366e9c1d"
          ]
        },
        "id": "FceXeiXQmyYo",
        "outputId": "5105fe00-bf5d-4df8-da17-ee8b9eb66812"
      },
      "source": [
        "# Initialize metrics\n",
        "bestValLoss = 100\n",
        "bestValValidPairs = 100\n",
        "print('Batch size:', dataset.batchSize())\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  averageEpochLoss  = tf.keras.metrics.Mean()\n",
        "  epochValidPairs   = tf.keras.metrics.Mean()\n",
        "  valValidPairs     = tf.keras.metrics.Mean()\n",
        "  progress = 0\n",
        "  trainSteps, valSteps = dataset.stepsInEpoch()\n",
        "\n",
        "  # Train\n",
        "  with tqdm(desc=f'Training - Epoch {epoch + 1}/{NUM_EPOCHS}', unit=' imgs', total= trainSteps * dataset.batchSize()) as pbar:\n",
        "    for X, y in dataset.trainDataset:                       # For each batch\n",
        "      loss, fraction = trainStep(X, y, model, optimizer)    \n",
        "      averageEpochLoss.update_state(loss)                   # Add current batch loss\n",
        "      epochValidPairs.update_state(fraction)                \n",
        "      # Update progress bar\n",
        "      progress += 1 / trainSteps\n",
        "      pbar.set_postfix(**{'Epoch loss': averageEpochLoss.result().numpy(), 'Epoch valid pairs': epochValidPairs.result().numpy(), 'Percentage of train set': progress})\n",
        "      pbar.update(dataset.batchSize())  # current batch size\n",
        "\n",
        "  # Validate\n",
        "  averageValLoss = tf.keras.metrics.Mean()\n",
        "  progress = 0\n",
        "  with tqdm(desc=f'Validation - Epoch {epoch + 1}/{NUM_EPOCHS}', unit=' imgs', total= valSteps*dataset.batchSize()) as pbar:\n",
        "      for X, y in dataset.valDataset:                       # For each batch\n",
        "          loss, fraction = valStep(X, y, model)\n",
        "          averageValLoss.update_state(loss)                 # Add current batch loss\n",
        "          valValidPairs.update_state(fraction)\n",
        "          # Update progress bar\n",
        "          progress += 1 / valSteps\n",
        "          pbar.set_postfix(**{ 'Prctg of validation set': progress})\n",
        "          pbar.update(dataset.batchSize())  # current batch size\n",
        "  #if averageValLoss.result().numpy() < bestValLoss:\n",
        "  if valValidPairs.result().numpy() < bestValValidPairs:    # Adjust based on validation metric\n",
        "      print(\"New best result on validation dataset - Saving model\")\n",
        "      bestValValidPairs = valValidPairs.result().numpy()\n",
        "      bestValLoss = averageValLoss.result().numpy()\n",
        "      model.save_weights(os.path.join(CKPT_DIR,f\"best.hdf5\")) \n",
        "\n",
        "  print(\"Epoch {:03d}: Train Loss: {:.3f} || Val Loss: {:.3f}\".format(epoch+1, averageEpochLoss.result(), averageValLoss.result()), end=\"\\n\\n\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aa4b6053f8940a98db05406204ea274",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Training - Epoch 1/1', max=50640.0, style=ProgressStyle(dâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfaaa229c1f34aa9b207ecdb83b13c23",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation - Epoch 1/1', max=1.0, styleâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "New best result on validation dataset - Saving model\n",
            "Epoch 001: Train Loss: 0.038 || Val Loss: 0.000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFOmfcdQleQz"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZrZB27qJcWK",
        "outputId": "752f21b1-ae8f-4e8b-b22f-2aabcd3a17dd"
      },
      "source": [
        "# Download the external lfw evaluation script:\n",
        "if not os.path.isdir('external'):\n",
        "  os.mkdir('external')\n",
        "!wget -O external/evaluate_lfw.py https://raw.githubusercontent.com/Jakub-Svoboda/DP/master/external/evaluate_lfw.py\n",
        "\n",
        "from external.evaluate_lfw import *"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-26 13:09:39--  https://raw.githubusercontent.com/Jakub-Svoboda/DP/master/external/evaluate_lfw.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12059 (12K) [text/plain]\n",
            "Saving to: â€˜external/evaluate_lfw.pyâ€™\n",
            "\n",
            "external/evaluate_l 100%[===================>]  11.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-26 13:09:40 (78.4 MB/s) - â€˜external/evaluate_lfw.pyâ€™ saved [12059/12059]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1dJSu_5ljK1"
      },
      "source": [
        "if VALLOSS == 'all' or VALLOSS == 'semiHard':      \n",
        "  testMetric = 1\n",
        "else:\n",
        "  testMetric = 0"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QtBBK5ErFTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e7d346-f524-4e14-ef93-f121bcce20ad"
      },
      "source": [
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "model = getNetwork(backbone = BACKBONE, embeddingSize=128, fcSize=1024, l2Norm=True)\n",
        "#model.load_weights('checkpoints/2021/03/02-09:56:15/epoch_49.hdf5')\n",
        "model.load_weights('./checkpoints/best.hdf5')\n",
        "# model.load_weights('/HOME/FaceNet/checkpoints/celeb_finetune/20200711-094426/best_epoch_19_weights.hdf5')\n",
        "# Run forward pass to calculate embeddings\n",
        "# Read the file containing the pairs used for testing\n",
        "evaluate_LFW(model,128,use_flipped_images=False,distance_metric=testMetric,verbose=2,N_folds=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/external/evaluate_lfw.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(pairs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Feed forward all pairs\n",
            "Calculating metrics\n",
            "Accuracy: 0.58267+-0.02318\n",
            "Validation rate: 0.00000+-0.00000 @ FAR=0.00000\n",
            "threshold : 0.00200+-0.00000\n",
            "Area Under Curve (AUC): 0.589\n",
            "Equal Error Rate (EER): 0.430\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.595     , 0.58333333, 0.58833333, 0.58833333, 0.62833333,\n",
              "       0.57833333, 0.585     , 0.545     , 0.54333333, 0.59166667])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq8cjZdF0c1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8f93daea-9c4e-4a52-d7f2-ae38c412e07f"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('./checkpoints/best.hdf5') "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5abb8717-eaf5-41d0-9084-2c2f76082576\", \"best.hdf5\", 11058840)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}